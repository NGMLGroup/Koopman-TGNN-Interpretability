{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from models.ESN import ESNModel\n",
    "from dataset.data_loaders import load_dataset, generate_datasets\n",
    "from models.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, L, F = load_dataset('spain')\n",
    "\n",
    "Xtr, Ytr, Xval, Yval, Xte, Yte, diffXte, diffYte = generate_datasets(data, L, F, device, test_percent = 0.25, val_percent = 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 32\n",
    "steps = Xtr[:,0].shape[0]\n",
    "\n",
    "X = torch.stack([Xtr[i:i+batch,0] for i in range(steps-batch+1)]).T.reshape((batch,steps-batch+1,1,1))\n",
    "Xv = Xval[:,0].reshape((1,Xval[:,0].shape[0],1,1))\n",
    "Xt = Xte[:,0].reshape((1,Xte[:,0].shape[0],1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.stack([Xtr[i:i+batch,0] for i in range(steps-batch+1)]).T.reshape((batch,steps-batch+1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 876, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([907, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "esn = ESNModel(batch, 500, 1, 0, 1, F).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x1 and 32x500)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mgphy\\GitHub_scripts\\Koopman\\DMD.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mgphy/GitHub_scripts/Koopman/DMD.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m esn(X)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\mgphy\\Anaconda3\\envs\\koopman\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mgphy\\GitHub_scripts\\Koopman\\models\\ESN.py:256\u001b[0m, in \u001b[0;36mESNModel.forward\u001b[1;34m(self, x, u, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39m# x: [batches steps nodes features]\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39m# u: [batches steps (nodes) features]\u001b[39;00m\n\u001b[0;32m    254\u001b[0m x \u001b[39m=\u001b[39m maybe_cat_exog(x, u)\n\u001b[1;32m--> 256\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreservoir(x, return_last_state\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    258\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadout(x)\n",
      "File \u001b[1;32mc:\\Users\\mgphy\\Anaconda3\\envs\\koopman\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mgphy\\GitHub_scripts\\Koopman\\models\\ESN.py:177\u001b[0m, in \u001b[0;36mReservoir.forward\u001b[1;34m(self, x, h0, return_last_state)\u001b[0m\n\u001b[0;32m    175\u001b[0m x_s \u001b[39m=\u001b[39m x[s]\n\u001b[0;32m    176\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreservoir_layers):\n\u001b[1;32m--> 177\u001b[0m     x_s \u001b[39m=\u001b[39m layer(x_s, h[i])\n\u001b[0;32m    178\u001b[0m     h_s\u001b[39m.\u001b[39mappend(x_s)\n\u001b[0;32m    179\u001b[0m \u001b[39m# update all states\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mgphy\\Anaconda3\\envs\\koopman\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mgphy\\GitHub_scripts\\Koopman\\models\\ESN.py:80\u001b[0m, in \u001b[0;36mReservoirLayer.forward\u001b[1;34m(self, x, h)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, h):\n\u001b[0;32m     79\u001b[0m     h_new \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\n\u001b[1;32m---> 80\u001b[0m         F\u001b[39m.\u001b[39;49mlinear(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb_ih) \u001b[39m+\u001b[39m F\u001b[39m.\u001b[39mlinear(h, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_hh))\n\u001b[0;32m     81\u001b[0m     h_new \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha) \u001b[39m*\u001b[39m h \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m*\u001b[39m h_new\n\u001b[0;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m h_new\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x1 and 32x500)"
     ]
    }
   ],
   "source": [
    "esn(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/100 [00:03<?, ?it/s, loss=0.00127]c:\\Users\\mgphy\\Anaconda3\\envs\\koopman\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([455])) that is different to the input size (torch.Size([907])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1:   0%|          | 0/100 [00:03<?, ?it/s, loss=0.00127]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (907) must match the size of tensor b (455) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mgphy\\GitHub_scripts\\Koopman\\DMD.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgphy/GitHub_scripts/Koopman/DMD.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Early stopping\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgphy/GitHub_scripts/Koopman/DMD.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m esn\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mgphy/GitHub_scripts/Koopman/DMD.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m valid_loss \u001b[39m=\u001b[39m torch_loss(esn(Xv)\u001b[39m.\u001b[39;49msqueeze(), Yval\u001b[39m.\u001b[39;49msqueeze())\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgphy/GitHub_scripts/Koopman/DMD.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# early_stopping needs the validation loss to check if it has decresed, \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgphy/GitHub_scripts/Koopman/DMD.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# and if it has, it will make a checkpoint of the current model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgphy/GitHub_scripts/Koopman/DMD.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m early_stopping(valid_loss, esn)\n",
      "File \u001b[1;32mc:\\Users\\mgphy\\Anaconda3\\envs\\koopman\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mgphy\\Anaconda3\\envs\\koopman\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\mgphy\\Anaconda3\\envs\\koopman\\lib\\site-packages\\torch\\nn\\functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3291\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3292\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[0;32m   3295\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\mgphy\\Anaconda3\\envs\\koopman\\lib\\site-packages\\torch\\functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[1;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (907) must match the size of tensor b (455) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "torch_optimizer = torch.optim.Adam(esn.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "epochs = 100\n",
    "torch_loss = torch.nn.MSELoss()\n",
    "\n",
    "# initialize the early_stopping object\n",
    "checkpoint_path = \"./checkpoints/QR/\"\n",
    "early_stopping = EarlyStopping(patience=20, verbose=False, path=checkpoint_path)\n",
    "\n",
    "with trange(epochs) as t:\n",
    "    for epoch in t:\n",
    "        esn.train()\n",
    "        torch_optimizer.zero_grad()\n",
    "        loss = torch_loss(esn(X).squeeze(), Ytr.squeeze())\n",
    "        loss.backward()\n",
    "        torch_optimizer.step()\n",
    "\n",
    "        # display progress bar\n",
    "        t.set_description(f\"Epoch {epoch+1}\")\n",
    "        t.set_postfix({\"loss\":float(loss / Ytr.shape[0])})\n",
    "\n",
    "        # Early stopping\n",
    "        esn.eval()\n",
    "        valid_loss = torch_loss(esn(Xv).squeeze(), Yval.squeeze()).item()\n",
    "\n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_loss, esn)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# load the last checkpoint with the best model\n",
    "esn.load_state_dict(torch.load(checkpoint_path + \"checkpoint.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1afd2e19310>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = esn(Xt).squeeze().detach().cpu()\n",
    "\n",
    "plt.plot(out)\n",
    "# plt.plot(Yte.squeeze().cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koopman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
